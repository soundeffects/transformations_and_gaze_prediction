@article{
    quianquiroga,
    author={Quian Quiroga, Rodrigo and Pedreira, Carlos},
    title={How Do We See Art: An Eye-Tracker Study},
    journal={Frontiers in Human Neuroscience},
    volume={5},
    year={2011},
    url={https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2011.00098},
    doi={10.3389/fnhum.2011.00098},
    ISSN={1662-5161},
}

@online{
    mit-tuebingen,
    author={
        Matthias K{\"u}mmerer and
        Zoya Bylinskii and
        Tilke Judd and
        Ali Borji and
        Laurent Itti and
        Fr{\'e}do Durand and
        Aude Oliva and
        Antonio Torralba and
        Matthias Bethge
    },
    title={MIT/Tübingen Saliency Benchmark},
    year={2024},
    url={https://saliency.tuebingen.ai/},
    urldate = {2025-03-15}
}

@inproceedings{
    made-easy,
    title={
        Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics
    },
    series={Lecture Notes in Computer Science},
    shorttitle={Saliency Benchmarking Made Easy},
    pages={798--814},
    booktitle={Computer Vision – {ECCV} 2018},
    publisher={Springer International Publishing},
    author={
        Matthias K{\"u}mmerer and
        Thomas S. A. Wallis and
        Matthias Bethge
    },
    editor={
        Vittorio Ferrari and
        Martial Hebert and
        Cristian Sminchisescu and
        Yair Weiss
    },
    date={2018},
}

@article{
    saliency-metrics,
    title={What do different evaluation metrics tell us about saliency models?},
    author={
        Zoya Bylinskii and
        Tilke Judd and
        Aude Oliva and
        Antonio Torralba and
        Fr{\'e}do Durand
    },
    journal={arXiv preprint arXiv:1604.03605},
    year={2016}
}

@inproceedings{
    mit300,
    title={
        A Benchmark of Computational Models of Saliency to Predict Human
        Fixations
    },
    author={Tilke Judd and Fr{\'e}do Durand and Antonio Torralba},
    booktitle={MIT Technical Report},
    year={2012}
}

@article{
    cat2000,
    title={
        CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research
    },
    author={Ali Borji and Laurent Itti},
    year={2015},
    eprint={1505.03581},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/1505.03581}, 
}

@article{
    information-gain,
    author={
        Matthias K{\"u}mmerer  and Thomas S. A. Wallis  and Matthias Bethge
    },
    title={Information-theoretic model comparison unifies saliency metrics},
    journal={Proceedings of the National Academy of Sciences},
    volume={112},
    number={52},
    pages={16054-16059},
    year={2015},
    doi={10.1073/pnas.1510393112},
    url={https://www.pnas.org/doi/abs/10.1073/pnas.1510393112},
    eprint={https://www.pnas.org/doi/pdf/10.1073/pnas.1510393112},
}

@article{
    gaze-transformations,
    title={How is Gaze Influenced by Image Transformations? Dataset and Model},
    volume={29},
    ISSN={1941-0042},
    url={http://dx.doi.org/10.1109/TIP.2019.2945857},
    doi={10.1109/tip.2019.2945857},
    journal={IEEE Transactions on Image Processing},
    publisher={Institute of Electrical and Electronics Engineers (IEEE)},
    author={
        Zhaohui Che and
        Ali Borji and
        Guangtao Zhai and
        Xiongkuo Min and
        Guodong Guo and
        Patrick Le Callet
    },
    year={2020},
    pages={2287–2300}
}

@misc{
    deepgazeiie,
    title={
        DeepGaze IIE: Calibrated prediction in and out-of-domain for
        state-of-the-art saliency modeling
    }, 
    author={
        Akis Linardos and
        Matthias K{\"u}mmerer and
        Ori Press and
        Matthias Bethge
    },
    year={2021},
    eprint={2105.12441},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    doi={10.48550/arXiv.2105.12441},
    url={https://arxiv.org/abs/2105.12441}, 
}

@article{
    annurev-vision,
    author = "Kümmerer, Matthias and Bethge, Matthias",
    title = "Predicting Visual Fixations", 
    journal= "Annual Review of Vision Science",
    year = "2023",
    volume = "9",
    number = "Volume 9, 2023",
    pages = "269-291",
    doi = "10.1146/annurev-vision-120822-072528",
    url = "https://www.annualreviews.org/content/journals/10.1146/annurev-vision-120822-072528",
    publisher = "Annual Reviews",
    issn = "2374-4650",
    type = "Journal Article",
    abstract = "As we navigate and behave in the world, we are constantly deciding, a few times per second, where to look next. The outcomes of these decisions in response to visual input are comparatively easy to measure as trajectories of eye movements, offering insight into many unconscious and conscious visual and cognitive processes. In this article, we review recent advances in predicting where we look. We focus on evaluating and comparing models: How can we consistently measure how well models predict eye movements, and how can we judge the contribution of different mechanisms? Probabilistic models facilitate a unified approach to fixation prediction that allows us to use explainable information explained to compare different models across different settings, such as static and video saliency, as well as scanpath prediction. We review how the large variety of saliency maps and scanpath models can be translated into this unifying framework, how much different factors contribute, and how we can select the most informative examples for model comparison. We conclude that the universal scale of information gain offers a powerful tool for the inspection of candidate mechanisms and experimental design that helps us understand the continual decision-making process that determines where we look.",
  }

@inbook{
    unisal,
    title={Unified Image and Video Saliency Modeling},
    ISBN={9783030585587},
    ISSN={1611-3349},
    url={http://dx.doi.org/10.1007/978-3-030-58558-7_25},
    DOI={10.1007/978-3-030-58558-7_25},
    booktitle={Computer Vision – ECCV 2020},
    publisher={Springer International Publishing},
    author={Droste, Richard and Jiao, Jianbo and Noble, J. Alison},
    year={2020},
    pages={419–435}
}