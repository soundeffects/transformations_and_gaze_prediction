\section{Introduction}
\label{sec:intro}

For the production of user interfaces, data visualizations, and visual media, predictions of human behavior can be very valuable. Currently, many producers will utilize A/B-testing in order to gain a sense of user preference. Gaze prediction for images could potentially be used to inform producers about viewers of the visual media will look (as a potential proxy for attention-to-detail). Gaze prediction is more granular and introspectable than A/B-testing. Some gaze prediction models can also return outputs in under ten milliseconds \cite{unisal}, which enables tight feedback loops and even on-the-fly program behavior directed by gaze prediction in interactive applications.

The field of gaze prediction has seen rapid progress with the emergence of deep learning models over the past decade. Still, when using deep learning models, users must be careful of possible hidden assumptions the model makes based on its training data. Studies which explore model performance on application-specific classes of images are sparse. Those which exist are out-of-date with the state-of-the-art models.

According to the methodology of state-of-the-art gaze prediction models, their training data is biased towards what we call "candid photography": minimally altered photographs. This is mostly due to transfer learning (a technique involving retraining from one domain to another) from the object recognition domain to the gaze prediction domain. Matthias KÃ¼mmerer and Matthias Bethge \cite{annurev-vision} show that all leading gaze prediction models utilize transfer learning. There is greater volume of object recognition data than gaze prediction data, and object recognition models assume candid photography inputs because detecting objects in real-world camera data (with minimal post-processing) is the most common use case.

Our applications of interest deal with stylized or abstract images, characterized by the use of computational generative and transformative techniques, in contrast to candid photography. The discrepancy between the majority of training data and expected input for these applications raises concern about gaze prediction models' performance on these tasks.

As a first step towards answering this concern, our work aims to quantify difference in performance between evaluation datasets and the transformations of those datasets for a set of common digital transformations. In the event that there is significant performance loss when transforming evaluation datasets, we will search for indicators which correlate with performance loss. This would let us make guesses about performance for transformations for which we have not collected data.

% \subsection{Stuff}
% \texttt{cvpr.sty}
% (\eg, this line is $087.5$)
% \textbf{make sure to update paper ID in the appropriate place in the tex file}
% \begin{equation}
%   E = m\cdot c^2
%   \label{eq:important}
% \end{equation}
% \url{http://www.pamitc.org/documents/mermin.pdf}.
% \begin{quote}
% \begin{center}
%      An analysis of the frobnicatable foo filter.
% \end{center}
%
%    In this paper, we present a performance analysis of the paper of Smith \etal [1], and show it to be inferior to all previously known methods.
%    Why the previous paper was accepted without this analysis is beyond me.
%
%    [1] Smith, L and Jones, C. ``The frobnicatable foo filter, a fundamental contribution to human knowledge''. Nature 381(12), 1-213.
% \end{quote}
%
% If you are making a submission to another conference at the same time that covers similar or overlapping material, you may need to refer to that submission to explain the differences, just as you would if you had previously published related work.
% In such cases, include the anonymized parallel submission~\cite{Authors14} as supplemental material and cite it as
% \begin{quote}
% [1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324, Supplied as supplemental material {\tt fg324.pdf}.
% \end{quote}
%
% Finally, you may feel you need to tell the reader that more details can be found elsewhere and refer them to a technical report.
% For conference submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a tech report for further details.
% Thus, you may say in the body of the paper ``further details may be found in~\cite{Authors14b}''.
% Then submit the tech report as supplemental material.
% Again, do not assume that the reviewers will read this material.
% \medskip
% \noindent
% {\bf Q:}
% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}
%
%    \caption{Example of caption.
%    It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
%    \label{fig:onecol}
% \end{figure}
%
% \subsection{Miscellaneous}
%
% \noindent
% Compare the following:\\
% \begin{tabular}{ll}
%  \verb'$conf_a$' &  $conf_a$ \\
%  \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
% \end{tabular}\\
% See The \TeX book, p165.
%%
%\begin{figure*}
%  \centering
%  \begin{subfigure}{0.68\linewidth}
%    \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%    \caption{An example of a subfigure.}
%    \label{fig:short-a}
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}{0.28\linewidth}
%    \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%    \caption{Another example of a subfigure.}
%    \label{fig:short-b}
%  \end{subfigure}
%  \caption{Example of a short caption, which should be centered.}
%  \label{fig:short}
%\end{figure*}
