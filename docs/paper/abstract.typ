= ABSTRACT
Deep learning gaze prediction models such as DeepGazeIIE are state-of-the-art for predicting where viewers will fixate on an image. However, their training data is biased towards "candid photography": minimally altered photographs for practical use. Problematically, many gaze prediction applications for digital media deal with stylized images. Our study shows that state-of-the-art gaze prediction models perform significantly worse on common image transformations, including cropping, rotation, contrast adjustment, and noise. We fail to find any indicators of the degradation of prediction accuracy for arbitrary transformations short of gathering human trial data. Our work emphasizes the need for more varied training data in order to confidently apply gaze prediction models to digital media.