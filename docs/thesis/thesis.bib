@article{
    quianquiroga,
    author={Rodrigo Quian Quiroga and Carlos Pedreira},
    title={How Do We See Art: An Eye-Tracker Study},
    journal={Frontiers in Human Neuroscience},
    volume={5},
    year={2011},
    url={https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2011.00098},
    doi={10.3389/fnhum.2011.00098},
    ISSN={1662-5161},
}

@online{
    mit-tuebingen,
    author={
        Matthias K{\"u}mmerer and
        Zoya Bylinskii and
        Tilke Judd and
        Ali Borji and
        Laurent Itti and
        Fr{\'e}do Durand and
        Aude Oliva and
        Antonio Torralba and
        Matthias Bethge
    },
    title={MIT/Tübingen Saliency Benchmark},
    year={2024},
    url={https://saliency.tuebingen.ai/},
    urldate = {2025-10-16}
}

@inproceedings{
    made-easy,
    title={
        Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics
    },
    series={Lecture Notes in Computer Science},
    shorttitle={Saliency Benchmarking Made Easy},
    pages={798--814},
    booktitle={Computer Vision – {ECCV} 2018},
    publisher={Springer International Publishing},
    author={
        Matthias K{\"u}mmerer and
        Thomas S. A. Wallis and
        Matthias Bethge
    },
    editor={
        Vittorio Ferrari and
        Martial Hebert and
        Cristian Sminchisescu and
        Yair Weiss
    },
    date={2018},
}

@article{
    saliency-metrics,
    author={Zoya Bylinskii and Tilke Judd and Aude Oliva and Antonio Torralba and Frédo Durand},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
    title={What Do Different Evaluation Metrics Tell Us About Saliency Models?}, 
    year={2019},
    volume={41},
    number={3},
    pages={740-757},
    doi={10.1109/TPAMI.2018.2815601}
}

@inproceedings{
    mit300,
    title={
        A Benchmark of Computational Models of Saliency to Predict Human
        Fixations
    },
    author={Tilke Judd and Fr{\'e}do Durand and Antonio Torralba},
    booktitle={MIT Technical Report},
    year={2012}
}

@article{
    cat2000,
    title={
        CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research
    },
    author={Ali Borji and Laurent Itti},
    year={2015},
    eprint={1505.03581},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/1505.03581}, 
}

@article{
    information-gain,
    author={
        Matthias K{\"u}mmerer  and Thomas S. A. Wallis  and Matthias Bethge
    },
    title={Information-theoretic model comparison unifies saliency metrics},
    journal={Proceedings of the National Academy of Sciences},
    volume={112},
    number={52},
    pages={16054-16059},
    year={2015},
    doi={10.1073/pnas.1510393112},
    url={https://www.pnas.org/doi/abs/10.1073/pnas.1510393112},
    eprint={https://www.pnas.org/doi/pdf/10.1073/pnas.1510393112},
}

@article{
    gaze-transformations,
    title={How is Gaze Influenced by Image Transformations? Dataset and Model},
    volume={29},
    ISSN={1941-0042},
    url={http://dx.doi.org/10.1109/TIP.2019.2945857},
    doi={10.1109/tip.2019.2945857},
    journal={IEEE Transactions on Image Processing},
    publisher={Institute of Electrical and Electronics Engineers (IEEE)},
    author={
        Zhaohui Che and
        Ali Borji and
        Guangtao Zhai and
        Xiongkuo Min and
        Guodong Guo and
        Patrick Le Callet
    },
    year={2020},
    pages={2287–2300}
}

@inproceedings{
    deepgazeiie,
    author={Akis Linardos and Matthias Kümmerer and Ori Press and Matthias Bethge},
    booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
    title={DeepGaze IIE: Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling}, 
    year={2021},
    pages={12899-12908},
    doi={10.1109/ICCV48922.2021.01268}
}

@article{
    annurev-vision,
    author = "Kümmerer, Matthias and Bethge, Matthias",
    title = "Predicting Visual Fixations", 
    journal= "Annual Review of Vision Science",
    year = "2023",
    volume = "9",
    pages = "269-291",
    doi = "10.1146/annurev-vision-120822-072528",
    url = "https://www.annualreviews.org/content/journals/10.1146/annurev-vision-120822-072528",
    publisher = "Annual Reviews",
    issn = "2374-4650",
    type = "Journal Article",
    abstract = "As we navigate and behave in the world, we are constantly deciding, a few times per second, where to look next. The outcomes of these decisions in response to visual input are comparatively easy to measure as trajectories of eye movements, offering insight into many unconscious and conscious visual and cognitive processes. In this article, we review recent advances in predicting where we look. We focus on evaluating and comparing models: How can we consistently measure how well models predict eye movements, and how can we judge the contribution of different mechanisms? Probabilistic models facilitate a unified approach to fixation prediction that allows us to use explainable information explained to compare different models across different settings, such as static and video saliency, as well as scanpath prediction. We review how the large variety of saliency maps and scanpath models can be translated into this unifying framework, how much different factors contribute, and how we can select the most informative examples for model comparison. We conclude that the universal scale of information gain offers a powerful tool for the inspection of candidate mechanisms and experimental design that helps us understand the continual decision-making process that determines where we look.",
  }

@inbook{
    unisal,
    title={Unified Image and Video Saliency Modeling},
    ISBN={9783030585587},
    ISSN={1611-3349},
    url={http://dx.doi.org/10.1007/978-3-030-58558-7_25},
    DOI={10.1007/978-3-030-58558-7_25},
    booktitle={Computer Vision – ECCV 2020},
    publisher={Springer International Publishing},
    author={Droste, Richard and Jiao, Jianbo and Noble, J. Alison},
    year={2020},
    pages={419–435}
}

@inproceedings{
    dhf1k,
    author = {Wang, Wenguan and Shen, Jianbing and Guo, Fang and Borji, Ali},
    year = {2018},
    month = {06},
    pages = {4894-4903},
    title = {Revisiting Video Saliency: A Large-Scale Benchmark and a New Model},
    doi = {10.1109/CVPR.2018.00514}
}

@InProceedings{
    salicon,
    author = {Jiang, Ming and Huang, Shengsheng and Duan, Juanyong and Zhao, Qi},
    title = {SALICON: Saliency in Context},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2015}
}

@inproceedings{
    aucjudd,
    author={Judd, Tilke and Ehinger, Krista and Durand, Frédo and Torralba, Antonio},
    booktitle={2009 IEEE 12th International Conference on Computer Vision}, 
    title={Learning to predict where humans look}, 
    year={2009},
    volume={},
    number={},
    pages={2106-2113},
    keywords={Layout;Application software;Predictive models;Spatial databases;Biological system modeling;Context modeling;Computer graphics;Human computer interaction;Image databases;Biology computing},
    doi={10.1109/ICCV.2009.5459462}
}

@article{
    sauc,
    title = {Visual correlates of fixation selection: effects of scale and time},
    journal = {Vision Research},
    volume = {45},
    number = {5},
    pages = {643-659},
    year = {2005},
    issn = {0042-6989},
    doi = {https://doi.org/10.1016/j.visres.2004.09.017},
    url = {https://www.sciencedirect.com/science/article/pii/S0042698904004626},
    author = {Benjamin W. Tatler and Roland J. Baddeley and Iain D. Gilchrist},
    keywords = {Saccadic selection, Image features, Spatial scale, Time course, Intermediate representation},
    abstract = {What distinguishes the locations that we fixate from those that we do not? To answer this question we recorded eye movements while observers viewed natural scenes, and recorded image characteristics centred at the locations that observers fixated. To investigate potential differences in the visual characteristics of fixated versus non-fixated locations, these images were transformed to make intensity, contrast, colour, and edge content explicit. Signal detection and information theoretic techniques were then used to compare fixated regions to those that were not. The presence of contrast and edge information was more strongly discriminatory than luminance or chromaticity. Fixated locations tended to be more distinctive in the high spatial frequencies. Extremes of low frequency luminance information were avoided. With prolonged viewing, consistency in fixation locations between observers decreased. In contrast to [Parkhurst, D. J., Law, K., & Niebur, E. (2002). Modeling the role of salience in the allocation of overt visual attention. Vision Research, 42 (1), 107–123] we found no change in the involvement of image features over time. We attribute this difference in our results to a systematic bias in their metric. We propose that saccade target selection involves an unchanging intermediate level representation of the scene but that the high-level interpretation of this representation changes over time.}
}

@article{
    nss,
    title = {Components of bottom-up gaze allocation in natural images},
    journal = {Vision Research},
    volume = {45},
    number = {18},
    pages = {2397-2416},
    year = {2005},
    issn = {0042-6989},
    doi = {https://doi.org/10.1016/j.visres.2005.03.019},
    url = {https://www.sciencedirect.com/science/article/pii/S0042698905001975},
    author = {Robert J. Peters and Asha Iyer and Laurent Itti and Christof Koch},
    keywords = {Salience, Attention, Eye movements, Contours},
    abstract = {Recent research [Parkhurst, D., Law, K., & Niebur, E., 2002. Modeling the role of salience in the allocation of overt visual attention. Vision Research 42 (1) (2002) 107–123] showed that a model of bottom-up visual attention can account in part for the spatial locations fixated by humans while free-viewing complex natural and artificial scenes. That study used a definition of salience based on local detectors with coarse global surround inhibition. Here, we use a similar framework to investigate the roles of several types of non-linear interactions known to exist in visual cortex, and of eccentricity-dependent processing. For each of these, we added a component to the salience model, including richer interactions among orientation-tuned units, both at spatial short range (for clutter reduction) and long range (for contour facilitation), and a detailed model of eccentricity-dependent changes in visual processing. Subjects free-viewed naturalistic and artificial images while their eye movements were recorded, and the resulting fixation locations were compared with the models’ predicted salience maps. We found that the proposed interactions indeed play a significant role in the spatiotemporal deployment of attention in natural scenes; about half of the observed inter-subject variance can be explained by these different models. This suggests that attentional guidance does not depend solely on local visual features, but must also include the effects of interactions among features. As models of these interactions become more accurate in predicting behaviorally-relevant salient locations, they become useful to a range of applications in computer vision and human-machine interface design.}
}

@article{
    cc,
    title = {Predicting visual fixations on video based on low-level visual features},
    journal = {Vision Research},
    volume = {47},
    number = {19},
    pages = {2483-2498},
    year = {2007},
    issn = {0042-6989},
    doi = {https://doi.org/10.1016/j.visres.2007.06.015},
    url = {https://www.sciencedirect.com/science/article/pii/S0042698907002593},
    author = {Olivier {Le Meur} and Patrick {Le Callet} and Dominique Barba},
    keywords = {Salience, Visual attention, Eye movements, Bottom–up, Top–down},
    abstract = {To what extent can a computational model of the bottom–up visual attention predict what an observer is looking at? What is the contribution of the low-level visual features in the attention deployment? To answer these questions, a new spatio-temporal computational model is proposed. This model incorporates several visual features; therefore, a fusion algorithm is required to combine the different saliency maps (achromatic, chromatic and temporal). To quantitatively assess the model performances, eye movements were recorded while naive observers viewed natural dynamic scenes. Four completing metrics have been used. In addition, predictions from the proposed model are compared to the predictions from a state of the art model [Itti’s model (Itti, L., Koch, C., & Niebur, E. (1998). A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence 20(11), 1254–1259)] and from three non-biologically plausible models (uniform, flicker and centered models). Regardless of the metric used, the proposed model shows significant improvement over the selected benchmarking models (except the centered model). Conclusions are drawn regarding both the influence of low-level visual features over time and the central bias in an eye tracking experiment.}
}

@article{
    emd,
    author = {Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas},
    year = {2000},
    month = {01},
    pages = {99-121},
    title = {The Earth Mover’s Distance as a metric for image retrieval},
    volume = {40},
    journal = {International Journal of Computer Vision}
}

@article{
    ssim,
    author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
    journal={IEEE Transactions on Image Processing}, 
    title={Image quality assessment: from error visibility to structural similarity}, 
    year={2004},
    volume={13},
    number={4},
    pages={600-612},
    keywords={Image quality;Humans;Transform coding;Visual system;Visual perception;Data mining;Layout;Quality assessment;Degradation;Indexes},
    doi={10.1109/TIP.2003.819861}
}

@online{
    our-code,
    author={
        James Youngblood
    },
    title={Transformations and Gaze Prediction},
    year={2025},
    url={https://codeberg.org/soundeffects/transformations_and_gaze_prediction},
    urldate = {2025-10-16}
}

@article{
    object-recognition-transforms,
    author={Malik, Girik and Crowder, Dakarai and Mingolla, Ennio},
    title={Extreme image transformations affect humans and machines differently},
    journal={Biological Cybernetics},
    year={2023},
    month={Oct},
    day={01},
    volume={117},
    number={4},
    pages={331-343},
    abstract={Some recent artificial neural networks (ANNs) claim to model aspects of primate neural and human performance data. Their success in object recognition is, however, dependent on exploiting low-level features for solving visual tasks in a way that humans do not. As a result, out-of-distribution or adversarial input is often challenging for ANNs. Humans instead learn abstract patterns and are mostly unaffected by many extreme image distortions. We introduce a set of novel image transforms inspired by neurophysiological findings and evaluate humans and ANNs on an object recognition task. We show that machines perform better than humans for certain transforms and struggle to perform at par with humans on others that are easy for humans. We quantify the differences in accuracy for humans and machines and find a ranking of difficulty for our transforms for human data. We also suggest how certain characteristics of human visual processing can be adapted to improve the performance of ANNs for our difficult-for-machines transforms.},
    issn={1432-0770},
    doi={10.1007/s00422-023-00968-7},
    url={https://doi.org/10.1007/s00422-023-00968-7}
}

